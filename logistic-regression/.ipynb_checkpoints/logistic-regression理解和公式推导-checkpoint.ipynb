{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 逻辑回归(logistic-regression)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">逻辑回归：个人理解就是一个线性回归经过阶跃函数的处理，变成一个二项分类器，输出结果只能是0，1的条件概率的大小，其实是一种概率模型。\n",
    "\n",
    "\n",
    "sigmoid函数：是一种阶跃函数(step function)，在不同横坐标尺度下，可以从0瞬间跳到1。从图形可以发现，当x>0，sigmoid函数值无限接近于1，反之接近于0。函数形式如下：$$ \\sigma(z) = \\frac{1}{1+e^{-\\omega^{T}x}} $$\n",
    "\n",
    "![](https://ws1.sinaimg.cn/large/006tNbRwly1fy9jvm1nuoj31920s4doo.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "___\n",
    "逻辑回归可以看成，在参数$\\theta$对已知$x$的条件下比较$ P(y=1| x;\\theta) $,$ P(y=0| x;\\theta) $概率大小，并选择较大的概率作为分类结果。\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "__核心在于：__\n",
    ">逻辑回归的公式-->$$ \\frac{1}{1+e^{-\\omega^{T}x}} $$\n",
    "用已知的数据集和训练样本$x_i$来训练这个模型，从而做到对未知的测试样本进行准确的估计和预测。所以需要对上述公式进行参数估计，求出$\\omega$的值\n",
    "\n",
    "$P_{y=1}=\\frac{1}{1+e^{-\\omega^T{x}}} = p$\n",
    "\n",
    "\n",
    "__参数估计：__\n",
    "* 损失函数：\n",
    "    * 线性回归-->最小二乘法\n",
    "    * 逻辑回归-->最大似然法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 最大似然估计(Maximum Likelihood Estimation)\n",
    "总体$X$为连续型，概率密度为$ f(x;\\theta),\\theta \\in \\Theta $,$ \\theta $是待估计参数，$ \\Theta $是$ \\theta$可能取值的范围，$X_{1},X_{2}....X_{n}$是来自$X$的样本,联合密度为$$\\prod_{i=1}^{n}f(x_{i},\\theta)$$\n",
    "随机点$X_{1},X_{2}...X_{n}$落在点$(x_{1},x_{2}...x_{n})$邻域内的概率近似为$$ \\prod_{i=1}^{n}f(x_{i},\\theta)dx_{i}$$\n",
    "这是一个关于$\\theta$的函数，值随$\\theta$变化，取估计值$\\hat\\theta$使得概率达到最大，由于因子$\\prod_{i=1}^{n}dx_{i}$不随 $\\theta$变化，所以只考虑函数\n",
    "$$L(\\theta)=L(x_{1},x_{2}...x_{n};\\theta)=\\prod_{i=1}^{n}f(x_{i};\\theta)$$的最大值，把$L(\\theta)$称为样本的似然函数，若$$L(x_{1},x_{2}...x_{n};\\hat\\theta)=\\max\\limits_{\\theta\\in\\Theta}L(x_{1},x_{2}...x_{n};\\theta)$$则把$\\hat\\theta(x_{1},x_{2}...x_{n})$作为$\\theta$的最大似然估计值，称$\\hat\\theta(X_{1},X_{2}...X_{n})$为$\\theta$的最大似然估计量。\n",
    "\n",
    "**求最大似然估计的步骤：**\n",
    "\n",
    "* 写出分布律或者概率密度函数$\\prod_{i=1}^{n}p(x_{i};\\theta)$或者$\\prod_{i=1}^{n}f(x_{i};\\theta)$\n",
    "\n",
    "* 写出似然函数$L(x_{1},x_{2}\\ldots,x_{n};\\hat\\theta)$\n",
    "\n",
    "* 对似然函数中的参数$\\theta$求偏导数\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**对于最大似然估计法：已经取到样本值$x_{1},x_{2}...x_{n}$,表明取到这一样本值的概率$L(\\theta)$比较大，所以不会考虑那些不能使样本$x_{1},x_{2}...x_{n}$出现的$\\theta \\in \\Theta$作为$\\theta$的估计，通过固定样本观察值$x_{1},x_{2}...x_{n}$，在$\\theta$可能取值范围$\\Theta$内挑选使得似然函数$ L(x_{1},x_{2}...x_{n};\\theta)$达到最大的参数$\\hat\\theta$作为估计值。**\n",
    "\n",
    "# 逻辑回归的损失函数\n",
    "当事件结果就只有2个标签时，$y_{n} \\in (0,1) $，把事件发生的概率看作$p$,那么标签1的概率为\n",
    "$$P_{y=1}=\\frac{1}{1+e^{-{w}^T{x}}} = p$$\n",
    "同理，标签0的概率为$P_{y=0}=1-p$\n",
    "所以这个事件发生的概率可以写成\n",
    "$$\n",
    "P(y \\mid x)=\n",
    "\\begin{cases}\n",
    "p,& y=1\\\\\n",
    "1-p,&y=0\\\\ \n",
    "\\end{cases}\n",
    "$$\n",
    "为了方便计算，也可以等价于：$$P(y_{i} \\mid x_{i}) = p^{y_{i}}(1-p)^{1-y_{i}}$$\n",
    "这个函数的含义是，在对于一个样本$(x_{i},y_{i})$，标签是$y_{i}$对应的概率是$P(y_{i} \\mid x_{i} = p^{y_{i}}(1-p)^{1-y_{i}})$，而对于一组数据，其样本概率为$$P_{总} = P(y_{1} \\mid x_{1})P(y_{2} \\mid x_{2})\\ldots P(y_{n} \\mid x_{n})=\\prod_{n=1}^Np^{y_{n}}(1-p)^{1-y_{n}}$$\n",
    "可以通过取对数，来简化计算，此外$P_{总}$是一个只包含$\\omega$一个未知数的函数：\n",
    "$$\n",
    "\\begin{aligned}\n",
    "F(\\omega)=ln(P_{总})&=ln(\\prod_{n=1}^Np^{y_{n}}(1-p)^{1-y_{n}})\\\\\n",
    "&=\\sum_{n=1}^{N}ln(p^{y_{n}}(1-p)^{1-y_{n}})\\\\\n",
    "&=\\sum_{n=1}^N(y_{n}ln(p)+(1-y_{n})ln(1-p))\\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "式子$(3)$是关于$\\omega$的一个函数，$P=\\frac{1}{1+e^{-{w}^T{x}}}$，可以通过改变$\\omega$的值来改变总概率$P_{总}$的大小。所以要使得$P_{总}$概率最大，只要选择一个$\\omega^{*}$使得概率最大即可。这种方法就是最大似然估计。\n",
    "\n",
    "**所以现在的问题从概率问题转化成了参数估计问题，以及一个求最优化的问题。**用数学语言描述就是:\n",
    "$$\\omega^{*}= arg\\max_{\\omega}F(\\omega)$$\n",
    "通常是采用梯度下降和拟牛顿法来解决这类问题。\n",
    "\n",
    "## 梯度\n",
    "关于梯度，对一个多维向量$x = (x_{1},x_{2}\\ldots,x_{n})$来讲，它的梯度就是分别对它每个分量求导数$x'=(x'_{1},x'_{2}\\ldots,x'_{n})$\n",
    "\n",
    "**推导过程**\n",
    "\n",
    "把$F(\\omega)$作为代价函数，[梯度下降法](https://www.jianshu.com/p/c7e642877b0e)的一般公式是：$$\\theta := \\theta - \\alpha\\cdot \\nabla f(\\omega)$$\n",
    "已知\n",
    "$$\n",
    "\\begin{cases}\n",
    "F(\\omega)=\\sum_{n=1}^N(y_{n}ln(p)+(1-y_{n})ln(1-p))\\\\\n",
    "p=\\frac{1}{1+e^{-{w}^T{x}}}\\\\\n",
    "\\end{cases}\n",
    "$$\n",
    "连立两式，求分别求导数可以得到解析式，对$p$用链式法则求导推导如下：$u=1+e^{-\\omega^{T}x},v=-\\omega^Tx,p=(\\frac{1}{u})'\\cdot v'$\n",
    "$$\n",
    "\\begin{aligned}\n",
    "p'&=-\\frac{1}{(1+e^{-\\omega^{T}x})^2}\\cdot(1+e^{-\\omega^{T}x})'\\\\\n",
    "&=-\\frac{1}{(1+e^{-\\omega^{T}x})^2}\\cdot e^{-\\omega^{T}x}\\cdot(-\\omega^{T}x)'\\\\\n",
    "&=-\\frac{1}{(1+e^{-\\omega^{T}x})^2}\\cdot e^{-\\omega^{T}x}\\cdot(-x)\\\\\n",
    "&=\\frac{1}{1+e^{-\\omega^{T}x}}\\cdot \\frac{e^{-\\omega^{T}x}}{1+e^{-\\omega^{T}x}}\\cdot x\\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "把$p$代入上式可以得到$p'=p(1-p)x$,同理$(1-p)'=-p(1-p)x$,之后可以对$F(\\omega)$求梯度，在求梯度前，需要知道几个结论$(Ax)'=A^T,(x^TA)' = A$可以在[这里](https://en.wikipedia.org/wiki/Matrix_calculus)查看：\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\nabla F(\\omega)'&=\\nabla(\\sum_{n=1}^N(y_{n}ln(p)+(1-y_{n})ln(1-p)))\\\\\n",
    "&=\\sum(y_{n}ln'(p)+(1-y_{n})ln'(1-p))\\\\\n",
    "&=\\sum((y_n\\frac1{p}p')+(1-y_n)\\frac1{1-p}(1-p))'(\\text此处可以把p'和(1-p)'代入)\\\\\n",
    "&=\\sum((y_n(1-p)x_n-(1-y_n)px_n)\\\\\n",
    "&=\\sum_{n=1}^{N}(y_n-p)x_n\\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "所以最终得到的梯度为：\n",
    "$\\nabla F(\\omega)=\\sum_{n=1}^{N}(y_n-p)x_n$\n",
    "\n",
    "之前已经知道，求解参数的方法一般采用梯度下降法，在获得梯度$\\nabla F(\\omega)$之后，由于这边是求能让$p$概率最大的$\\omega$，那么选择梯度上升算法。迭代步骤为：$$\\theta:=\\theta+\\nabla F(\\omega)$$\n",
    "展开后等于：\n",
    "$$\\theta:=\\theta+\\sum_{n=1}^{N}(y_n-p)x_n;p=\\frac{1}{1+e^{-\\omega^{T}x}}$$\n",
    "\n",
    "## 随机梯度下降\n",
    "梯度下降求导每次都用了所有样本点参与梯度计算，随机梯度下降的做法则是随机算则一个样本点$(x_i,y_i)$来代表整体，使得$E(G(\\omega))=\\nabla F(\\omega)$,所以迭代过程为：$$\\theta:=\\theta+\\alpha N(y_n-p)x_n;p=\\frac{1}{1+e^{-\\omega^{T}x}}$$\n",
    "而$\\alpha N$也都是常数，所以：$$\\theta:=\\theta+\\alpha(y_n-p)x_n;p=\\frac{1}{1+e^{-\\omega^{T}x}}$$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    ">需要补充的数学知识:\n",
    "* 复合函数求导-->链式法则\n",
    "* 最大似然估计\n",
    "* 期望和无偏估计\n",
    "\n",
    "___\n",
    "update 18.12.20\n",
    "\n",
    "关于梯度下降，有好多类型：\n",
    "* __批量梯度下降(Batch Gradient Descent,BGD):__用所有样本来更新参数\n",
    "* __随机梯度下降(Stochastic Gradient Descent,SGD):__随机选择一个样本来更新参数，但是解不一定是最优解\n",
    "* __小批量梯度下降(Mini-batch Gradient Descent,MBGD):__选择小批量样本来更新参数。\n",
    "\n",
    ">其中的门道数不胜数，感慨一句，学无止境。\n",
    "\n",
    "# 小结\n",
    "**对于逻辑回归而言，其实际就是一个$sigmoid$函数套上一个回归模型，对于输入$X$有输出$y \\in (0,1)$，所以只需要对其中的参数进行估计即可，对于参数估计问题，涉及到代价函数，而逻辑回归的代价函数可以用最大似然估计得到。因为从概率的角度来讲，对于已知的样本，通常认为在某一参数下取到这些样本的概率一定是比较大的，所以可以通过挑选参数集合中最大的参数取值来使得估计概率最大。最后可以采用梯度下降法，为了提高效率也可以使用随机梯度下降来求参数，因为这里是求最大值，只需要将梯度方向符号改为+即可。\n",
    "**\n",
    "# 参考阅读：\n",
    "1. [逻辑回归原理小结](https://www.cnblogs.com/pinard/p/6029432.html).刘建平\n",
    "2. 《概率论与数理统计》.盛骤\n",
    "3. 《机器学习》.周志华\n",
    "4. [逻辑回归公式推导](https://www.zhihu.com/search?type=content&q=逻辑回归)\n",
    "5. [关于文中公式排版以及输入](https://math.meta.stackexchange.com/questions/5020/mathjax-basic-tutorial-and-quick-reference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
